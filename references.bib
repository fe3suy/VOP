@book{brown2014magnetic,
  author    = {Brown, Robert W. and Cheng, Y.-C. Norman and Haacke, E. Mark and Thompson, Michael R. and Venkatesan, Ramesh},
  title     = {Magnetic Resonance Imaging: {P}hysical {P}rinciples and {S}equence {D}esign},
  year      = {2014},
  publisher = {John Wiley \& Sons}
}

@misc{mrimaster2024,
  author       = {Mrimaster},
  title        = {{TR} and {TE} in {MRI} — {TR} (repetition time), {TE} (echo time) and image contrast},
  year         = {2024},
  month        = {June},
  url          = {https://mrimaster.com/tr-and-te-in-mri/},
  note         = {Accessed: 2025-03-25}
}


@inproceedings{ronneberger2015unet,
  author    = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  title     = {U-{N}et: {C}onvolutional {N}etworks for {B}iomedical {I}mage {S}egmentation},
  booktitle = {Medical Image Computing and Computer-Assisted Intervention (MICCAI)},
  year      = {2015},
  pages     = {234--241},
  doi       = {10.1007/978-3-319-24574-4_28},
  url       = {https://doi.org/10.1007/978-3-319-24574-4_28}
}

@misc{bruker2025pharmascan,
  author       = {Bruker},
  title        = {{PharmaScan} — {P}reclinical {MRI}},
  year         = {2025},
  url          = {https://www.bruker.com/en/products-and-solutions/preclinical-imaging/mri/pharmascan-new.html},
  note         = {Accessed: 2025-03-25}
}

@article{grover2015mri,
  author  = {Grover, Vijay P. B. and Tognarelli, Joshua M. and Crossey, Mary M. E. and Cox, I. Jane and Taylor-Robinson, Simon D. and McPhail, Mark J. W.},
  title   = {Magnetic {R}esonance {I}maging: {P}rinciples and {T}echniques: {L}essons for {C}linicians},
  journal = {Journal of Clinical and Experimental Hepatology},
  volume  = {5},
  number  = {3},
  pages   = {246--255},
  year    = {2015},
  month   = {August},
  doi     = {10.1016/j.jceh.2015.08.001},
  url     = {https://doi.org/10.1016/j.jceh.2015.08.001}
}

@article{chavhan2009t2star,
  author  = {Chavhan, Govind B. and Babyn, Paul S. and Thomas, Bejoy and Shroff, Manohar M. and Haacke, E. Mark},
  title   = {Principles, {T}echniques, and {A}pplications of {T2*}-based {MR} {I}maging and {I}ts {S}pecial {A}pplications},
  journal = {Radiographics},
  volume  = {29},
  number  = {5},
  pages   = {1433--1449},
  year    = {2009},
  month   = {September},
  doi     = {10.1148/rg.295095034},
  url     = {https://doi.org/10.1148/rg.295095034}
}


@misc{tran2025unet,
  author       = {Tran, Minh},
  title        = {Understanding {U}-{N}et},
  year         = {2025},
  month        = {January},
  url          = {https://towardsdatascience.com/understanding-u-net-61276b10f360/},
  note         = {Accessed: 2025-03-25}
}

@inproceedings{verhelst2025denoising,
  author    = {Verhelst, Niels and Courtens, Joran and Vervenne, Boris and Abi Akl, Maya and Maebe, Jens and Lajtos, Melissa and Vandenberghe, Stefaan and Vanhove, Christian and Muller, Florence Marie},
  title     = {Deep {L}earning for {D}enoising and {S}uper-{R}esolution in {L}ow-{R}esolution {M}icro-{MRI} for {M}ouse {I}maging: {A}chieving {S}horter {S}can {T}imes},
  booktitle = {Proceedings of the 20th European Molecular Imaging Meeting (EMIM)},
  year      = {2025},
  pages     = {1}
}
@inproceedings{10.1145/3292500.3330701,
author = {Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
title = {Optuna: A Next-generation Hyperparameter Optimization Framework},
year = {2019},
isbn = {9781450362016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292500.3330701},
doi = {10.1145/3292500.3330701},
abstract = {The purpose of this study is to introduce new design-criteria for next-generation hyperparameter optimization software. The criteria we propose include (1) define-by-run API that allows users to construct the parameter search space dynamically, (2) efficient implementation of both searching and pruning strategies, and (3) easy-to-setup, versatile architecture that can be deployed for various purposes, ranging from scalable distributed computing to light-weight experiment conducted via interactive interface. In order to prove our point, we will introduce Optuna, an optimization software which is a culmination of our effort in the development of a next generation optimization software. As an optimization software designed with define-by-run principle, Optuna is particularly the first of its kind. We will present the design-techniques that became necessary in the development of the software that meets the above criteria, and demonstrate the power of our new design through experimental results and real world applications. Our software is available under the MIT license (https://github.com/pfnet/optuna/).},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
pages = {2623–2631},
numpages = {9},
keywords = {machine learning system, hyperparameter optimization, black-box optimization, Bayesian optimization},
location = {Anchorage, AK, USA},
series = {KDD '19}
}

 @misc{unknown-author-no-date,
	title = {{Tutorials}},
	url = {https://www.overleaf.com/learn/latex/Tutorials},
}

@misc{massed-compute-2025,
	author = {Massed Compute},
	month = {2},
	title = {{FAQ Answers - Massed compute}},
	year = {2025},
	url = {https://massedcompute.com/faq-answers/?question=What%20are%20the%20key%20differences%20between%20Adam%20and%20SGD%20optimizers%20in%20large%20language%20model%20training?},
}

@article{sanghvirajit-2025,
	author = {Sanghvirajit},
	month = {3},
	title = {{Everything you need to know about Adam and RMSprop Optimizer}},
	year = {2025},
	url = {https://medium.com/analytics-vidhya/a-complete-guide-to-adam-and-rmsprop-optimizer-75f4502d83be},
}

@misc{pykes-2021,
	author = {Pykes, Kurtis},
	month = {10},
	title = {{AdamW Optimizer in PyTorch Tutorial}},
	year = {2021},
	url = {https://www.datacamp.com/tutorial/adamw-optimizer-in-pytorch},
}

@article{dhanushkumar-2023,
	author = {DhanushKumar},
	month = {11},
	title = {{MAX POOLING - DhanushKumar - Medium}},
	year = {2023},
	url = {https://medium.com/@danushidk507/max-pooling-ef545993b6e4},
}

@misc{unknown-author-no-date2,
	title = {{7.3. Padding and Stride — Dive into Deep Learning 1.0.3 documentation}},
	url = {https://d2l.ai/chapter_convolutional-neural-networks/padding-and-strides.html},
}

@misc{isbhargav-2020,
	author = {Isbhargav},
	month = {7},
	title = {{Guide to Pytorch Learning Rate Scheduling}},
	year = {2020},
	url = {https://www.kaggle.com/code/isbhargav/guide-to-pytorch-learning-rate-scheduling},
}

@misc{unknown-author-no-date3,
	title = {{Comprehensive overview of learning rate schedulers in Machine Learning | CloudFactory Computer Vision Wiki}},
	url = {https://wiki.cloudfactory.com/docs/mp-wiki/scheduler},
}

@misc{unknown-author-2025,
	month = {4},
	title = {{Bicubic Interpolation | CloudInary}},
	year = {2025},
	url = {https://cloudinary.com/glossary/bicubic-interpolation},
}

@article{amanrao-2023,
	author = {Amanrao},
	month = {9},
	title = {{Image Upscaling using Bicubic Interpolation - Amanrao - Medium}},
	year = {2023},
	url = {https://medium.com/@amanrao032/image-upscaling-using-bicubic-interpolation-ddb37295df0},
}

@misc{jordan_2018_setting,
  author = {Jordan, Jeremy},
  month = {03},
  title = {Setting the learning rate of your neural network.},
  url = {https://www.jeremyjordan.me/nn-learning-rate/},
  year = {2018},
  organization = {Jeremy Jordan}
}

@misc{anderson_2023_loss,
  author = {Anderson, Martin},
  month = {02},
  title = {Loss Functions in Machine Learning - Metaphysic.ai},
  url = {https://blog.metaphysic.ai/loss-functions-in-machine-learning/},
  urldate = {2025-04-13},
  year = {2023},
  organization = {Metaphysic.ai -}
}

@misc{bharatiya_2019_comprehensive,
  author = {Bharatiya, Pronod},
  month = {09},
  title = {Comprehensive Guide to the ReLU Activation Function in Neural Networks: Definition, Role, and Type Explained},
  url = {https://data-intelligence.hashnode.dev/comprehensive-guide-to-the-relu-activation-function-in-neural-networks-definition-role-and-type-explained},
  year = {2019},
  organization = {Pronod Bharatiya's Blog}
}

@online{mseJim,
  author       = {Jim Frost},
  title        = {Mean Squared Error (MSE)},
  year         = {2023},
  url          = {https://statisticsbyjim.com/regression/mean-squared-error-mse/},
  note         = {Accessed: 2025-04-30}
}

@article{dosselmann-2009,
	author = {Dosselmann, Richard and Yang, Xue Dong},
	journal = {Signal Image and Video Processing},
	month = {11},
	number = {1},
	pages = {81--91},
	title = {{A comprehensive assessment of the structural similarity index}},
	volume = {5},
	year = {2009},
	doi = {10.1007/s11760-009-0144-1},
	url = {https://doi.org/10.1007/s11760-009-0144-1},
}

@INPROCEEDINGS{9036442,
  author={Zaheer, Raniah and Shaziya, Humera},
  booktitle={2019 Third International Conference on Inventive Systems and Control (ICISC)}, 
  title={A Study of the Optimization Algorithms in Deep Learning}, 
  year={2019},
  volume={},
  number={},
  pages={536-539},
  keywords={Optimization;Training;Convergence;Testing;Conferences;Control systems;Deep learning;Optimization Algorithm;Stochastic Gradient Descent;Momentum;RMSprop;AdamOptimizer;MNIST;FashionMNIST;Cifar10;Cifar100},
  doi={10.1109/ICISC44355.2019.9036442}}


@article{Dastmalchi,
author = {Dastmalchi, Hamidreza and Aghaeinia, Hassan},
year = {2022},
month = {06},
pages = {116755},
title = {Super-resolution of very low-resolution face images with a wavelet integrated, identity preserving, adversarial network},
volume = {107},
journal = {Signal Processing: Image Communication},
doi = {10.1016/j.image.2022.116755}
}


@Article{relu,
AUTHOR = {Maniatopoulos, Andreas and Mitianoudis, Nikolaos},
TITLE = {Learnable Leaky ReLU (LeLeLU): An Alternative Accuracy-Optimized Activation Function},
JOURNAL = {Information},
VOLUME = {12},
YEAR = {2021},
NUMBER = {12},
ARTICLE-NUMBER = {513},
URL = {https://www.mdpi.com/2078-2489/12/12/513},
ISSN = {2078-2489},
ABSTRACT = {In neural networks, a vital component in the learning and inference process is the activation function. There are many different approaches, but only nonlinear activation functions allow such networks to compute non-trivial problems by using only a small number of nodes, and such activation functions are called nonlinearities. With the emergence of deep learning, the need for competent activation functions that can enable or expedite learning in deeper layers has emerged. In this paper, we propose a novel activation function, combining many features of successful activation functions, achieving 2.53% higher accuracy than the industry standard ReLU in a variety of test cases.},
DOI = {10.3390/info12120513}
}

@ARTICLE{1284395,
  author={Zhou Wang and Bovik, A.C. and Sheikh, H.R. and Simoncelli, E.P.},
  journal={IEEE Transactions on Image Processing}, 
  title={Image quality assessment: from error visibility to structural similarity}, 
  year={2004},
  volume={13},
  number={4},
  pages={600-612},
  keywords={Image quality;Humans;Transform coding;Visual system;Visual perception;Data mining;Layout;Quality assessment;Degradation;Indexes},
  doi={10.1109/TIP.2003.819861}}

@article{SNR-validation,
author = {Chavhan, Govind and Babyn, Paul and Thomas, Bejoy and Shroff, Manohar and Haacke, Mark},
year = {2009},
month = {09},
pages = {1433-49},
title = {Principles, Techniques, and Applications of T2*-based MR Imaging and Its Special Applications},
volume = {29},
journal = {Radiographics : a review publication of the Radiological Society of North America, Inc},
doi = {10.1148/rg.295095034}
}
@misc{zhang2018unreasonableeffectivenessdeepfeatures,
      title={The Unreasonable Effectiveness of Deep Features as a Perceptual Metric}, 
      author={Richard Zhang and Phillip Isola and Alexei A. Efros and Eli Shechtman and Oliver Wang},
      year={2018},
      eprint={1801.03924},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1801.03924}
}